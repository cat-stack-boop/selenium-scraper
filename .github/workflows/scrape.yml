name: Scrape ChatGPT with Selenium

on:
  schedule:
    - cron: "0 2 * * *"  # Run daily at 2 AM UTC
  workflow_dispatch:      # Manual trigger available

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    env:
      PYTHON_VERSION: "3.10"  # Specify exact version
      
    steps:
      - name: Check out the repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 2  # Fetch last 2 days for comparison

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'  # Enable pip caching

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 python-dotenv
          
      - name: Set up Chrome and ChromeDriver
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver
          sudo ln -s /usr/bin/chromedriver /usr/local/bin/chromedriver
          sudo ln -s /usr/bin/chromium-browser /usr/local/bin/chromium-browser
          
      - name: Create .env file
        run: |
          echo "CHROME_DRIVER_PATH=/usr/bin/chromedriver" >> .env
          echo "WEBSITE_URL=https://chat.openai.com" >> .env
          echo "WAIT_TIMEOUT=20" >> .env
          echo "REPO_PATH=." >> .env
          echo "COMPARISON_OUTPUT=changes.json" >> .env

      - name: Run Selenium Scraper
        run: python scrape_chatgpt.py
        continue-on-error: true  # Continue even if comparison fails

      - name: Check for changes
        id: check-changes
        run: |
          if [ -f changes.json ]; then
            echo "CHANGES_FOUND=$(cat changes.json | jq 'any_values')" >> $GITHUB_ENV
          else
            echo "CHANGES_FOUND=false" >> $GITHUB_ENV
          fi

      - name: Commit and Push Changes
        if: success()
        run: |
          git config --global user.email "feeltheagi@proton.me"
          git config --global user.name "Adam"
          git add scraped_pages/ changes.json
          
          if [ "${{ env.CHANGES_FOUND }}" == "true" ]; then
            git commit -m "Automated Selenium scrape - Changes detected"
          else
            git commit -m "Automated Selenium scrape - No changes"
          fi
          
          git push

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: scrape-results
          path: |
            scraped_pages/
            changes.json
            scraper.log
          retention-days: 7

      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'Scraper workflow failed',
              body: `Workflow run failed: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`
            })